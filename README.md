# `ğŸ“ˆ DyCodeEval`

This repository contains the main implementation of **DyCodeEval**, introduced in our ICML 2025 paper:
*â€œDyCodeEval: Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination.â€*

[ğŸ† Leaderboard](https://your-leaderboard-link) â€¢ [ğŸ’» Code](https://github.com/your-username/DyCodeEval) â€¢ [ğŸ¤— Hugging Face Dataset](https://huggingface.co/datasets/your-dataset) â€¢ [ğŸ”® Code Kaleidoscope Project](https://github.com/your-username/DyCodeEval/tree/main/kaleidoscope)


## Introducation 

**DyCodeEval** proposes a novel **dynamic benchmarking framework** for evaluating code large language models (Code LLMs). It leverages a **multi-agent cooperation** strategy to **rewrite existing benchmarks** at evaluation time, producing programming problems that are: 1. Semantically equivalent, 2. Diverse, and 3. Non-deterministic. This dynamic generation process helps **mitigate data contamination** and provides a more robust and faithful assessment of a model's reasoning capabilities.

### Design Oveerview
<div  align="center">    
 <img src="https://github.com/SeekingDream/DyCodeEval/blob/main/resource/dycodeeval_overview.jpg" width="760" height="310" alt="Design Overview"/><br/>
</div>   

### ğŸ”§ This repository provides:

* Core implementation of DyCodeEval to dynamically rewrite existing programming problems
* Scripts to reproduce all experiments and benchmarks presented in the paper
* Pre-generated benchmark variants for **HumanEval** and **MBPP**


## How to Run

### Installation



## File Structure







