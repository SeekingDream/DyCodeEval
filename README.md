# `üìà DyCodeEval`

This repository contains the main implementation of **DyCodeEval**, introduced in our ICML 2025 paper:
*‚ÄúDyCodeEval: Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination.‚Äù*

**DyCodeEval** proposes a novel **dynamic benchmarking framework** for evaluating code large language models (Code LLMs). It leverages a **multi-agent cooperation** strategy to **rewrite existing benchmarks** at evaluation time, producing programming problems that are: 1. Semantically equivalent, 2. Diverse, and 3. Non-deterministic. This dynamic generation process helps **mitigate data contamination** and provides a more robust and faithful assessment of a model's reasoning capabilities.



## Design Oveerview
<div  align="center">    
 <img src="https://github.com/SeekingDream/DyCodeEval/blob/main/resource/dycodeeval_overview.jpg" width="760" height="310" alt="Design Overview"/><br/>
</div>   

## How to Run

## File Structure

