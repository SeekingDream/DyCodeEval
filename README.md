# `📈 DyCodeEval`

This repository contains the main implementation of **DyCodeEval**, introduced in our ICML 2025 paper:
*“DyCodeEval: Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination.”*

[🏆 Leaderboard](https://your-leaderboard-link) • [💻 Code](https://github.com/your-username/DyCodeEval) • [🤗 Hugging Face Dataset](https://huggingface.co/datasets/your-dataset) • [🔮 Code Kaleidoscope Project](https://github.com/your-username/DyCodeEval/tree/main/kaleidoscope)


## Introducation 

**DyCodeEval** proposes a novel **dynamic benchmarking framework** for evaluating code large language models (Code LLMs). It leverages a **multi-agent cooperation** strategy to **rewrite existing benchmarks** at evaluation time, producing programming problems that are: 1. Semantically equivalent, 2. Diverse, and 3. Non-deterministic. This dynamic generation process helps **mitigate data contamination** and provides a more robust and faithful assessment of a model's reasoning capabilities.

### Design Oveerview
<div  align="center">    
 <img src="https://github.com/SeekingDream/DyCodeEval/blob/main/resource/dycodeeval_overview.jpg" width="760" height="310" alt="Design Overview"/><br/>
</div>   

### 🔧 This repository provides:

* Core implementation of DyCodeEval to dynamically rewrite existing programming problems
* Scripts to reproduce all experiments and benchmarks presented in the paper
* Pre-generated benchmark variants for **HumanEval** and **MBPP**


## How to Run

### Installation



## File Structure







